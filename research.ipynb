{"cells":[{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"}},"source":["![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n","<hr>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from AlgorithmImports import *\n","import numpy as np\n","import pandas as pd\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","\n","qb = QuantBook()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["spy = qb.add_equity(\"SPY\", Resolution.DAILY).symbol\n","history = qb.history(spy, datetime(2019, 1, 1), datetime(2024, 1, 1))\n","\n","history.head()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["closes = history['close'].droplevel(0)\n","returns = np.log(closes / closes.shift(1)).dropna()\n","\n","print(f\"\\nReturns statistics:\")\n","print(f\"  Mean:     {returns.mean():.6f}\")\n","print(f\"  Std:      {returns.std():.6f}\")\n","print(f\"  Skew:     {returns.skew():.4f}\")\n","print(f\"  Kurtosis: {returns.kurtosis():.4f}\")  # Should be > 3"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def estimate_hurst(series, min_window=10, max_window=None):\n","    if isinstance(series, pd.Series):\n","        series = series.values\n","    series = np.asarray(series, dtype=np.float64)\n","    \n","    n = len(series)\n","    if n < 4 * min_window:\n","        return 0.5\n","    \n","    if max_window is None:\n","        max_window = n // 4\n","    \n","    mean = np.mean(series)\n","    y = np.cumsum(series - mean)\n","    \n","    window_sizes = []\n","    fluctuations = []\n","    \n","    for window in range(min_window, max_window + 1):\n","        n_segments = n // window\n","        if n_segments < 2:\n","            continue\n","            \n","        f2 = [] \n","        for i in range(n_segments):\n","            start = i * window\n","            end = start + window\n","            segment = y[start:end]\n","            \n","            # Fit linear trend\n","            x = np.arange(window)\n","            coeffs = np.polyfit(x, segment, 1)\n","            trend = np.polyval(coeffs, x)\n","            \n","            # Compute variance of detrended segment\n","            detrended = segment - trend\n","            f2.append(np.mean(detrended ** 2))\n","        \n","        # RMS fluctuation\n","        fluctuation = np.sqrt(np.mean(f2))\n","        \n","        window_sizes.append(window)\n","        fluctuations.append(fluctuation)\n","    \n","    if len(window_sizes) < 5:\n","        return 0.5\n","    \n","    log_windows = np.log(window_sizes)\n","    log_fluct = np.log(fluctuations)\n","    \n","    slope, _, r_value, _, _ = stats.linregress(log_windows, log_fluct)\n","    return float(np.clip(slope, 0.0, 1.0))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["print(\"HURST EXPONENT VALIDATION\")\n","print(\"=\" * 60)\n","print(f\"{'Method':<25} {'Random Walk':<15} {'Trending':<15} {'Mean-Rev':<15}\")\n","print(\"-\" * 60)\n","\n","np.random.seed(42)\n","n_samples = 2000\n","\n","random_walk = np.random.randn(n_samples)\n","\n","\n","persistent = []\n","x = 0\n","for i in range(n_samples):\n","    x = 0.3 * x + np.random.randn()\n","    persistent.append(x)\n","persistent = np.cumsum(persistent)  # Integrate to get FBM-like series\n","persistent_returns = np.diff(persistent)\n","\n","mean_rev = []\n","x = 0\n","for i in range(n_samples):\n","    x = -0.3 * x + np.random.randn()  # Anti-persistence\n","    mean_rev.append(x)\n","mean_rev_returns = np.array(mean_rev)\n","\n","h_rs_random = estimate_hurst(random_walk)\n","h_rs_trend = estimate_hurst(persistent_returns)\n","h_rs_meanrev = estimate_hurst(mean_rev_returns)\n","\n","print(f\"{'R/S (DFA)':<25} {h_rs_random:<15.3f} {h_rs_trend:<15.3f} {h_rs_meanrev:<15.3f}\")\n","\n","print(\"-\" * 60)\n","print(f\"{'Expected':<25} {'~0.50':<15} {'>0.50':<15} {'<0.50':<15}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"\\nREAL MARKET DATA (SPY)\")\n","print(\"=\" * 40)\n","\n","h_dfa_spy = estimate_hurst(returns.values)\n","print(f\"DFA:              H = {h_dfa_spy:.3f}\")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# =============================================================================\n","# CELL 1: Hill Estimator Implementation\n","# =============================================================================\n","\n","def estimate_tail_index(returns, threshold_pct=10, min_exceedances=20):\n","    \"\"\"Estimate tail index α using the Hill estimator.\n","    \n","    The tail index α characterizes the heaviness of distribution tails:\n","    - α = 2: Gaussian (thin tails)\n","    - α < 2: Fat tails (Mandelbrot regime)\n","    - α ≈ 1.7: Typical for financial returns (Mandelbrot's finding)\n","    - α < 1: Extremely fat tails (infinite mean)\n","    \n","    Lower α = fatter tails = more extreme events than Gaussian predicts.\n","    \n","    Args:\n","        returns: Array of returns (will use absolute values)\n","        threshold_pct: Percentage of largest observations to use (default 10%)\n","        min_exceedances: Minimum number of tail observations required\n","        \n","    Returns:\n","        Tuple of (alpha, standard_error, n_exceedances)\n","    \"\"\"\n","    if isinstance(returns, pd.Series):\n","        returns = returns.values\n","    returns = np.asarray(returns, dtype=np.float64)\n","    \n","    # Use absolute returns (we care about tail magnitude, not direction)\n","    abs_returns = np.abs(returns)\n","    abs_returns = abs_returns[~np.isnan(abs_returns)]\n","    abs_returns = abs_returns[abs_returns > 1e-10]  # Remove zeros\n","    \n","    n = len(abs_returns)\n","    if n < min_exceedances * 2:\n","        return 2.0, np.nan, 0  # Default to Gaussian if insufficient data\n","    \n","    # Number of tail observations to use\n","    k = max(min_exceedances, int(n * threshold_pct / 100))\n","    k = min(k, n - 1)  # Can't use all observations\n","    \n","    # Sort in descending order and get the k largest\n","    sorted_returns = np.sort(abs_returns)[::-1]\n","    tail_observations = sorted_returns[:k]\n","    threshold = sorted_returns[k]  # The k-th largest value\n","    \n","    if threshold <= 0:\n","        return 2.0, np.nan, 0\n","    \n","    # Hill estimator: α = k / Σ log(X_i / X_k)\n","    log_ratios = np.log(tail_observations / threshold)\n","    \n","    # Remove any infinite or nan values\n","    log_ratios = log_ratios[np.isfinite(log_ratios)]\n","    log_ratios = log_ratios[log_ratios > 0]  # Must be positive\n","    \n","    if len(log_ratios) < min_exceedances:\n","        return 2.0, np.nan, len(log_ratios)\n","    \n","    # Hill estimate\n","    alpha = len(log_ratios) / np.sum(log_ratios)\n","    \n","    # Standard error (asymptotic)\n","    se = alpha / np.sqrt(len(log_ratios))\n","    \n","    # Clip to reasonable range\n","    alpha = float(np.clip(alpha, 0.5, 5.0))\n","    \n","    return alpha, se, len(log_ratios)\n","\n","\n","def estimate_tail_index_both_tails(returns, threshold_pct=10, min_exceedances=20):\n","    \"\"\"Estimate tail index separately for left and right tails.\n","    \n","    Returns:\n","        Dictionary with 'left' (negative returns), 'right' (positive returns), \n","        and 'combined' tail indices.\n","    \"\"\"\n","    if isinstance(returns, pd.Series):\n","        returns = returns.values\n","    returns = np.asarray(returns, dtype=np.float64)\n","    returns = returns[~np.isnan(returns)]\n","    \n","    # Split into positive and negative\n","    positive_returns = returns[returns > 0]\n","    negative_returns = -returns[returns < 0]  # Make positive for Hill estimator\n","    \n","    alpha_right, se_right, n_right = estimate_tail_index(\n","        positive_returns, threshold_pct, min_exceedances\n","    )\n","    alpha_left, se_left, n_left = estimate_tail_index(\n","        negative_returns, threshold_pct, min_exceedances\n","    )\n","    alpha_combined, se_combined, n_combined = estimate_tail_index(\n","        returns, threshold_pct, min_exceedances\n","    )\n","    \n","    return {\n","        'left': {'alpha': alpha_left, 'se': se_left, 'n': n_left},\n","        'right': {'alpha': alpha_right, 'se': se_right, 'n': n_right},\n","        'combined': {'alpha': alpha_combined, 'se': se_combined, 'n': n_combined}\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# =============================================================================\n","# CELL 2: Generate Synthetic Test Distributions\n","# =============================================================================\n","np.random.seed(42)\n","n_samples = 10000  # Large sample for accurate estimation\n","\n","# Distribution 1: Gaussian (α should be high, ~3-4 or higher)\n","# Note: Gaussian technically has α = ∞, but Hill estimator gives finite values\n","gaussian_samples = np.random.randn(n_samples) * 0.01\n","\n","# Distribution 2: Student's t with df=3 (α should be ≈ 3)\n","from scipy.stats import t as t_dist\n","t3_samples = t_dist.rvs(df=3, size=n_samples) * 0.01\n","\n","# Distribution 3: Student's t with df=2 (α should be ≈ 2)\n","t2_samples = t_dist.rvs(df=2, size=n_samples) * 0.01\n","\n","# Distribution 4: Pareto-like / Power law with α = 1.5 (very fat tails)\n","# Using inverse transform: if U ~ Uniform(0,1), then X = u^(-1/α) follows Pareto\n","def generate_pareto_tails(n, alpha_true):\n","    \"\"\"Generate samples with Pareto tails (power law).\"\"\"\n","    u = np.random.uniform(0.01, 1, n)  # Avoid zero\n","    signs = np.random.choice([-1, 1], n)\n","    return signs * (u ** (-1 / alpha_true)) * 0.01\n","\n","pareto_15_samples = generate_pareto_tails(n_samples, alpha_true=1.5)\n","pareto_20_samples = generate_pareto_tails(n_samples, alpha_true=2.0)\n","pareto_25_samples = generate_pareto_tails(n_samples, alpha_true=2.5)\n","\n","print(\"Synthetic distributions generated:\")\n","print(f\"  Gaussian:    n={n_samples}, expected α > 3\")\n","print(f\"  Student t3:  n={n_samples}, expected α ≈ 3.0\")\n","print(f\"  Student t2:  n={n_samples}, expected α ≈ 2.0\")\n","print(f\"  Pareto 1.5:  n={n_samples}, expected α ≈ 1.5\")\n","print(f\"  Pareto 2.0:  n={n_samples}, expected α ≈ 2.0\")\n","print(f\"  Pareto 2.5:  n={n_samples}, expected α ≈ 2.5\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# =============================================================================\n","# CELL 3: Validate Hill Estimator on Synthetic Data\n","# =============================================================================\n","print(\"HILL ESTIMATOR VALIDATION\")\n","print(\"=\" * 70)\n","print(f\"{'Distribution':<20} {'Expected α':<12} {'Estimated α':<12} {'Std Err':<10} {'Status'}\")\n","print(\"-\" * 70)\n","\n","test_cases = [\n","    (\"Gaussian\", gaussian_samples, \">3.0\"),\n","    (\"Student t (df=3)\", t3_samples, \"≈3.0\"),\n","    (\"Student t (df=2)\", t2_samples, \"≈2.0\"),\n","    (\"Pareto α=1.5\", pareto_15_samples, \"≈1.5\"),\n","    (\"Pareto α=2.0\", pareto_20_samples, \"≈2.0\"),\n","    (\"Pareto α=2.5\", pareto_25_samples, \"≈2.5\"),\n","]\n","\n","for name, samples, expected in test_cases:\n","    alpha, se, n_exc = estimate_tail_index(samples, threshold_pct=10)\n","    \n","    # Determine if estimate is reasonable\n","    if \">\" in expected:\n","        status = \"✓\" if alpha > 3.0 else \"✗\"\n","    elif \"1.5\" in expected:\n","        status = \"✓\" if 1.2 < alpha < 1.8 else \"✗\"\n","    elif \"2.0\" in expected:\n","        status = \"✓\" if 1.7 < alpha < 2.3 else \"✗\"\n","    elif \"2.5\" in expected:\n","        status = \"✓\" if 2.2 < alpha < 2.8 else \"✗\"\n","    elif \"3.0\" in expected:\n","        status = \"✓\" if 2.7 < alpha < 3.5 else \"✗\"\n","    else:\n","        status = \"?\"\n","    \n","    print(f\"{name:<20} {expected:<12} {alpha:<12.3f} {se:<10.3f} {status}\")\n","\n","print(\"-\" * 70)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# =============================================================================\n","# CELL 4: Sensitivity Analysis - Effect of threshold_pct\n","# =============================================================================\n","print(\"\\nSENSITIVITY TO THRESHOLD PERCENTAGE\")\n","print(\"=\" * 70)\n","print(\"Testing Pareto α=2.0 distribution with different threshold percentages:\")\n","print(\"-\" * 70)\n","print(f\"{'Threshold %':<15} {'Estimated α':<15} {'Std Error':<15} {'N exceedances'}\")\n","print(\"-\" * 70)\n","\n","for thresh in [5, 10, 15, 20, 25, 30]:\n","    alpha, se, n_exc = estimate_tail_index(pareto_20_samples, threshold_pct=thresh)\n","    print(f\"{thresh:<15} {alpha:<15.3f} {se:<15.3f} {n_exc}\")\n","\n","print(\"-\" * 70)\n","print(\"Note: Lower threshold % uses fewer extreme observations (less bias, more variance)\")\n","print(\"      Higher threshold % uses more observations (more bias, less variance)\")"]}],"metadata":{"kernelspec":{"display_name":"Foundation-Autogluon","language":"python","name":"foundation-autogluon"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.14"}},"nbformat":4,"nbformat_minor":2}